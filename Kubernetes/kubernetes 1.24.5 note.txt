
########################
kubernetes 1.24.5 note
########################

	-- kubernetes 최소 사양 -> CPU*2, Memory*4G

	kevin / k8spass#

	# 서버 구성
	1) hostname	k8s-master
	2) IP		192.168.56.100 / 255.255.255.0 / 192.168.56.2 / 8.8.8.8
	3) package설치	(apt -y update) vim, openssh-server
	4) putty, winSCP 연결 완료.



1. OS 환경 구성

	보안 구성 해제 -> firewall-cmd / Selinux
	swap 해제? kubernetes는 swap을 사용하지 않는다 -> container = process -> Pod
	network forward 설정

kevin@k8s-master:~$ sudo apt -y install firewalld
kevin@k8s-master:~$ sudo systemctl daemon-reload
kevin@k8s-master:~$ sudo systemctl disable firewalld.service
kevin@k8s-master:~$ sudo systemctl stop firewalld.service
kevin@k8s-master:~$ sudo firewall-cmd --reload
FirewallD is not running

kevin@k8s-master:~$ sudo swapoff -a
kevin@k8s-master:~$ sudo vi /etc/fstab
#UUID=bd02c5f5-075d-4c7f-af70-4d60c97c9437 none            swap    sw      0       0

kevin@k8s-master:~$ sudo apt -y install ntp
kevin@k8s-master:~$ sudo systemctl daemon-reload
kevin@k8s-master:~$ sudo systemctl enable ntp
kevin@k8s-master:~$ sudo systemctl restart ntp
kevin@k8s-master:~$ sudo systemctl status ntp
kevin@k8s-master:~$ sudo ntpq -p

kevin@k8s-master:~$ sudo su -
root@k8s-master:~# cat /proc/sys/net/ipv4/ip_forward
0
root@k8s-master:~# echo '1' > /proc/sys/net/ipv4/ip_forward
root@k8s-master:~# cat /proc/sys/net/ipv4/ip_forward
1

[ERROR FileContent--proc-sys-net-ipv4-ip_forward]: /proc/sys/net/ipv4/ip_forward contents are not set to 1

=============================================================

2. docker install -> containerd (cri-o) -> ~ 1.24.5

sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common gnupg2

curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key --keyring /etc/apt/trusted.gpg.d/docker.gpg add -

sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

tail /etc/apt/sources.list
...							  (확인)
deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable
# deb-src [arch=amd64] https://download.docker.com/linux/ubuntu focal stable

sudo apt -y update

kevin@k8s-master:~$ apt-cache policy docker-ce
docker-ce:
  Installed: (none)
  Candidate: 5:20.10.18~3-0~ubuntu-focal
  Version table:
     5:20.10.18~3-0~ubuntu-focal 500
        500 https://download.docker.com/linux/ubuntu focal/stable amd64 Packages

sudo apt-get -y install docker-ce
sudo docker version

kevin@k8s-master:~$ sudo docker info | grep -i cgroup
 Cgroup Driver: cgroupfs
		ㄴ 자원 할당 -> 제한 -> request & limit
		ㄴ kubernetes는 cgroup을 사용하지 않고, 상위 process "systemd"를 사용해야 한다.

kevin@k8s-master:~$ sudo vi /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}

kevin@k8s-master:~$ sudo mkdir -p /etc/systemd/system/docker.service.d
kevin@k8s-master:~$ sudo systemctl daemon-reload
kevin@k8s-master:~$ sudo systemctl enable docker
kevin@k8s-master:~$ sudo systemctl restart docker
kevin@k8s-master:~$ sudo systemctl status docker

kevin@k8s-master:~$ sudo docker info | grep -i cgroup
 Cgroup Driver: systemd



3. kubernetes tool

	# from kubernetes repo.

	kubeadm	-> bootstrap -> init(초기화) -> worker node JOIN -> upgrade
	kubectl -> CLI
	kubelet -> process(daemon)

	sudo vi /etc/hosts



	
kevin@k8s-master:~$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

kevin@k8s-master:~$ cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
> deb https://apt.kubernetes.io/ kubernetes-xenial main
> EOF

kevin@k8s-master:~$ grep kubernetes /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main

kevin@k8s-master:~$ sudo apt -y update

kevin@k8s-master:~$ sudo apt-cache policy kubeadm
kubeadm:
  Installed: (none)
  Candidate: 1.25.2-00

			1.24.5-00
			1	Major
			24	Minor
			5	patchset

kevin@k8s-master:~$ sudo apt-cache policy kubelet | grep 1.24

kevin@k8s-master:~$ sudo apt -y install kubeadm=1.24.5-00 kubelet=1.24.5-00 kubectl=1.24.5-00

kevin@k8s-master:~$ sudo apt list | grep kubernetes

kevin@k8s-master:~$ sudo systemctl daemon-reload
kevin@k8s-master:~$ sudo systemctl enable --now kubelet

kevin@k8s-master:~$ sudo vi /etc/hosts
127.0.0.1       localhost
127.0.1.1       k8s-master
192.168.56.100  k8s-master
192.168.56.101  k8s-node1
192.168.56.102  k8s-node2
192.168.56.103  k8s-node3


	-> 복제 * 3
		k8s-node1 / k8s-node2 / k8s-node3

	-> IP 및 hostname 변경

	-> k8s-master / k8s-node1 / k8s-node2 만 부팅하고, putty 연결 -> k8s-node3는 확장용



# container runtime

kevin@k8s-master:/etc/containerd$ ls
config.toml
kevin@k8s-master:/etc/containerd$ sudo mv config.toml config.toml.org

kevin@k8s-master:~$ sudo systemctl restart containerd.service
kevin@k8s-master:~$ sudo systemctl restart kubelet




kevin@k8s-master:~$ sudo kubeadm init --pod-network-cidr=10.96.0.0/12 --apiserver-advertise-address=192.168.56.100
I0929 14:03:24.628820    6322 version.go:255] remote version is much newer: v1.25.2; falling back to: stable-1.24
[init] Using Kubernetes version: v1.24.6
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.56.100]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.56.100 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.56.100 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 27.004676 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: si8krc.rlj6gkhbtaq0ih45
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.56.100:6443 --token si8krc.rlj6gkhbtaq0ih45 \
        --discovery-token-ca-cert-hash sha256:33658b007ada8c8dc1c7468605e44f966a0260e4989e90a413b6b4b2977af10d



kevin@k8s-master:~$ sudo apt -y install net-tools

kevin@k8s-master:~$ mkdir -p $HOME/.kube
kevin@k8s-master:~$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
kevin@k8s-master:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
kevin@k8s-master:~$ sudo netstat -nlp | grep LISTEN

kevin@k8s-master:~$ sudo netstat -nlp | grep LISTEN
tcp        0      0 192.168.56.100:2379     0.0.0.0:*               LISTEN      7187/etcd
tcp        0      0 192.168.56.100:2380     0.0.0.0:*               LISTEN      7187/etcd
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      570/systemd-resolve
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      736/sshd: /usr/sbin
tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      7468/kube-proxy
tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      7331/kubelet
tcp        0      0 127.0.0.1:10257         0.0.0.0:*               LISTEN      7160/kube-controlle
tcp        0      0 127.0.0.1:10259         0.0.0.0:*               LISTEN      7178/kube-scheduler
tcp        0      0 127.0.0.1:2381          0.0.0.0:*               LISTEN      7187/etcd
tcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      7187/etcd
tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      609/cupsd
tcp        0      0 127.0.0.1:35635         0.0.0.0:*               LISTEN      5845/containerd
tcp6       0      0 :::10250                :::*                    LISTEN      7331/kubelet
tcp6       0      0 :::22                   :::*                    LISTEN      736/sshd: /usr/sbin
tcp6       0      0 :::10256                :::*                    LISTEN      7468/kube-proxy
tcp6       0      0 :::6443                 :::*                    LISTEN      7213/kube-apiserver




kevin@k8s-master:~$ sudo apt -y install bash-completion
kevin@k8s-master:~$ source <(kubectl completion bash)
kevin@k8s-master:~$ echo "source <(kubectl completion bash)" >> .bashrc


# 각 노드에서 수행
sudo kubeadm join 192.168.56.100:6443 --token si8krc.rlj6gkhbtaq0ih45 \
        --discovery-token-ca-cert-hash sha256:33658b007ada8c8dc1c7468605e44f966a0260e4989e90a413b6b4b2977af10d

# master
kevin@k8s-master:~$ kubectl get po --all-namespaces
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE
kube-system   coredns-6d4b75cb6d-98cnc             0/1     Pending   0          11m
kube-system   coredns-6d4b75cb6d-c4t8f             0/1     Pending   0          11m
kube-system   etcd-k8s-master                      1/1     Running   0          11m
kube-system   kube-apiserver-k8s-master            1/1     Running   0          11m
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          11m
kube-system   kube-proxy-4xfmj                     1/1     Running   0          11m
kube-system   kube-proxy-bf6zp                     1/1     Running   0          117s
kube-system   kube-proxy-f275d                     1/1     Running   0          114s
kube-system   kube-scheduler-k8s-master            1/1     Running   0          11m

kevin@k8s-master:~$ kubectl get po -A

kevin@k8s-master:~$ kubectl describe po kube-proxy-4xfmj -n kube-system



# join 실패 시 모든 노드(master 포함)에서 아래 명령 수행
sudo kubeadm reset
sudo systemctl restart kubelet


kevin@k8s-master:~$ kubectl apply -f calico.yaml
kevin@k8s-master:~$ kubectl delete -f calico.yaml











4. CNI -> calico plugin -> cluster network (L3)










5. dashboard -> PKI 인증서 -> windows-certmgr.msc
	+ prometheus & exporter & grafana







sudo docker pull kubernetesui/dashboard:v2.6.1
sudo docker pull kubernetesui/metrics-scraper:v1.0.8

kevin@k8s-master:~$ kubectl delete -f recommended.yaml
kevin@k8s-master:~$ kubectl apply -f recommended.yaml



	1.23.x ~	-> dashboard token 제공
	=================================================
	1.24.x ~	-> dashboard token 직접 생성해서 사용



	rolebind -> cluster-admin의 *을 kubernetes-dashboard에 연결(rolebinding)

	token 생성

	인증서 생성 -> client


dashboard_token$ grep 'client-certificate-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d >> kubecfg.crt

dashboard_token$ grep 'client-key-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d >> kubecfg.key

dashboard_token$ openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name "kubernetes-admin"
Enter Export Password: (k8spass#)
Verifying - Enter Export Password:

dashboard_token$ cp /etc/kubernetes/pki/ca.crt ./

dashboard_token$ ls

------- winSCP를 이용해서 windowsd의 c:\k8s\dashboard_token 이전

# certmgr.msc에 등록 -> powershell 관리자로 열기
PS C:\k8s\dashboard_token> certutil.exe -addstore "Root" ca.crt

PS C:\k8s\dashboard_token> certutil.exe -p k8spass# -user -importPFX .\kubecfg.p12




---------------------------- day02 ----------------------------------


	VM-k8s		1.24.5 -> 1.24.6 "upgrade"

		ㄴ dashboard -> PKI 인증서 기반의 접근 방법 -> 6443(apiserver)

	GKE		1.22.12
		ㄴ http://localhost:8001
				     ㄴ kubernetes proxy port -> kubectl proxy
	v2.4
	ㄴ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.4.0/aio/deploy/recommended.yaml

C:\>kubectl get po -A

	ㄴ 




	# Kubernetes Components -> 5 binary components
		
	1) kube-apiserver-k8s-master

	2) etcd-k8s-master

	3) kube-scheduler-k8s-master

	4) kube-controller-manager-k8s-master

	5) kubelet

	6) kube-proxy

	7) caclico







kevin@k8s-master:~/LABs$ sudo apt -y install rdate
kevin@k8s-master:~/LABs$ sudo rdate -s time.bora.net
kevin@k8s-master:~/LABs$ date


git clone https://github.com/brayanlee/k8s-prometheus.git



	# k8s object = k8s api-resource
	# Pod, ReplicaSet, Deployment, DaemonSet, Job, CronJob, ...

	kubectl api-resources | grep -i network

	# yaml code 구성, CLI (kubectl run -> CKA) 

	kubectl {create | apply | delete} -f pod.yaml
		
		- create	yaml에 지정된 object 생성, update X
		- apply		yaml에 지정된 object 생성, update(일부)
		- delete	yaml에 지정된 object 삭제

	kubectl {get | describe} {object type} object_name [-n namespace | default]



kevin@k8s-master:~$ kubectl -n kube-system exec -it etcd-k8s-master -- sh \
> -c "ETCDCTL_API=3 \
> ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt \
> ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt \
> ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key \
> etcdctl endpoint health"
127.0.0.1:2379 is healthy: successfully committed proposal: took = 121.973296ms

kevin@k8s-master:~$ kubectl -n kube-system exec -it etcd-k8s-master -- sh \
> -c "ETCDCTL_API=3 etcdctl \
> --cacert=/etc/kubernetes/pki/etcd/ca.crt \
> --cert=/etc/kubernetes/pki/etcd/server.crt \
> --key=/etc/kubernetes/pki/etcd/server.key \
> member list"
76335a6259872c1a, started, k8s-master, https://192.168.56.100:2380, https://192.168.56.100:2379, false


snapshot -> 복제

kubectl -n kube-system exec -it etcd-k8s-master -- sh \
-c "ETCDCTL_API=3 etcdctl \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /var/lib/etcd/snapshot.db"

kevin@k8s-master:~$ sudo ls -lh /var/lib/etcd
total 5.1M
drwx------ 4 root root   29  9월 29 16:39 member
-rw------- 1 root root 5.1M  9월 30 12:26 snapshot.db

kevin@k8s-master:~$ sudo cp /var/lib/etcd/snaphost.db $HOME/backup/snapshot.db~$(date +%m-%d-%y)

kubectl -n kube-system exec -it etcd-k8s-master -- sh \
-c "ETCDCTL_API=3 etcdctl \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot restore /var/lib/etcd/snapshot.db"




	# kubernetes master 

	kubeadm init --pod-network-cidr=10.96.0.0/12 --advertise-address=192.168.56.100

	... kubeadm join ~

		-> addon


	# calico network plugin -> 여러개의 interface(tool), module로 구성되어 있음.
		ㄴ calico.yaml -> 4700 line
		ㄴ pod network 
calico-kube-controllers-6799f5f4b4-kdqp4
# DaemonSet (pod를 모든 노드에 하나씩 배포)
calico-node-89htx
calico-node-gtxw5
calico-node-hcv68
		-> 라우팅 정보를 이용한 network 수행 <--- etcd에 저장




kevin@k8s-master:~/LABs/mynode$ vi runapp.js
var http = require('http');
var content = function(req, resp) {
 resp.end("Welcome to Kubernetes~! -본인이름-" + "\n");
 resp.writeHead(200);
}
var web = http.createServer(content);
web.listen(8000);

kevin@k8s-master:~/LABs/mynode$ vi Dockerfile
FROM node:slim
EXPOSE 8000
COPY runapp.js .
CMD node runapp.js

-- build -> docker run TEST! -> tag -> login -> push
		ㄴ sudo docker run -it --name=mynode -p 10000:8000 본인아이디/mynode:1.0 
		-> curl localhost:10000
apiVersion: v1
kind: Pod
metadata:
  name: mynode-pod1			---> kubectl apply -f mynode-pod1.yaml
spec:					---> kubectl get po -o wide
  containers:				---> curl POD-IP:8000
  - name: mynode-container		---> network 정보 확인
    image: 본인아이디/mynode:1.0
    ports:
    - containerPort: 8000





--------------------------- day03 ---------------------------------------


	# kubernetes api resource = object

	 application -> container -> Pod (기본 배포 단위) -> ReplicaSet -> Deployment

	
	# Pod vs. Service = docker run -p
	
		-> web service
		-> DB 						    VPN -> 개발자, 운영자	
		    ㄴ RDBMS(SQL) -> MySQL -> Pod + Service(client, workbench)
			ㄴ 제약사항? 3306, 환경변수-> MYSQL_ROOT_PASSWORD

		    ㄴ NoSQL -> mongoDB -> Pod + Service(client, Robo3T)
			ㄴ 제약사항? 27017, JSON구조
		
		  -> 생성 및 실행 중 오류가 발생하면? kubectl logs -f pod_name
		  -> docker pull {mysql:5.7 | mongo:4.0}




	# Pod -> kubernetes 운영, 관리 -> CKA

	- Pod -> 고래, 물개 등의 작은 무리(떼) -> container(app)

		-> 하나의 Pod는 여러 개의 container를 보유할 수 있다. -> Multi-container 생성
		-> Pod는 container의 host와 같다. -> PodIP를 통해 container에 접근할 수 있다.
		-> 배포 단위

		-> container type
			1) 일반적인 application container 	-> runtime container
			2) 특정 조건을 담고 있는 container 	-> init container -> 충족 시 runtime container
			3) runtime container의 log 수집을 위한 container -> sidecar container
				ㄴ sidecar, Ambassader, Adapter 설계 기법
	
		-> 추상화 object
			-> 격리된 container를 포함
			-> 격리된 network(networkk ns), storage(namespace) 자원등을 공유
		

	
-> mydb-pod에 접속
-> item db 생성 -> products -> prod_id, prod_name -> insert -> select
===========
-> workbench에 연결 후 조회


root@k8s-node1:/proc/sys/net/ipv4/conf# route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
10.109.131.0    k8s-node2       255.255.255.192 UG    0      0        0 tunl0


-- node2
10.111.156.64   k8s-node1       255.255.255.192 UG    0      0        0 tunl0	-> IPIP 





	# Pod - Label

		- key: value 구조로 작성, 여러개 지정이 가능, 복합적 의미 지정
		- Pod를 식별
		- pod1, pod2, pod3 -> 동일한 label 부여 -> service 연결 -> 그룹화, LB
	
		- kubectl get po --show-labels
		- kubectl get po -l key
		- kubectl get po --selector=key=value

		- node에도 label 설정이 유용

		-> label의 확장 기능을 제공 -> "annotations"
			ㄴ 추가적인 메타 정보 제공


	# Pod - schedule -> kube-scheduler-k8s-master에 의해 특정 계산 알고리즘으로 노드에 할당(지정)됨.

		- nodeName -> hostname으로 노드를 지정
				
		spec:
		  nodeName: k8s-node1

		===> "kubelet"을 통해 직접 할당하는 방식

	--------------------------------------------------------

		- nodeSelector -> 해당 노드의 label을 통해 지정

		spec:
		  nodeSelector:
		    kubernetes.io/hostname: k8s-node2

		===> control plane.scheduler에 의해 할당 되는 방식 

	--------------------------------------------------------

		- 사용자 지정 방식 -> label


		시나리오) 우리회사는 150대의 k8s cluster를 운영 중이다. 이중 140대는 worker node로 사용 중이다.
			ㄴ 특정 worker node는 disktype=ssd 사용 중 이다.
			ㄴ "disktype=ssd"인 worker node에 pod를 지정하고자 한다.	-> k8s-node1
			ㄴ 또는, GPU로 운영 중("gpu=true")인 worker node를 pod에 지정해야 한다. -> k8s-node2

	kevin@k8s-master:~/LABs/labels$ kubectl label nodes k8s-node2 gpu=true
	kevin@k8s-master:~/LABs/labels$ kubectl get no k8s-node2 --show-labels
	kevin@k8s-master:~/LABs/labels$ cp pod-label-1.yaml node-label-2.yaml

	spec:
	  nodeSelector:
            gpu: "true"


	-- 삭제 -> kevin@k8s-master:~/LABs/labels$ kubectl label nodes k8s-node1 disktype-

















 	